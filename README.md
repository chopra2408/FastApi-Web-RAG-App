# AI-Powered Web Scraper & Query System

## ğŸš€ Overview
This project is an **AI-powered web scraper and query system** that processes web pages, extracts text, and allows users to ask questions based on the extracted content. The system leverages **FastAPI** for the backend, **FAISS** for vector storage, and **Groq's Llama 3.2 model** for AI-powered responses.

## âœ¨ Features
- ğŸŒ **Scrape Web Pages** â€“ Extracts text from any given URL.
- ğŸ” **AI-Powered Search** â€“ Queries processed content using Llama 3.2.
- âš¡ **Fast & Efficient** â€“ Uses FAISS for quick similarity search.
- ğŸ¨ **Modern UI** â€“ Dark-themed, responsive, and interactive frontend.
- ğŸš€ **Asynchronous Processing** â€“ Ensures smooth user experience.

## ğŸ› ï¸ Tech Stack
### **Backend**
- **FastAPI** â€“ Handles API requests.
- **BeautifulSoup** â€“ Scrapes web content.
- **LangChain & FAISS** â€“ Splits and stores extracted content.
- **Groq API** â€“ Provides AI-powered responses.

### **Frontend**
- **HTML, CSS, JavaScript** â€“ Built for an interactive experience.
- **Dark Theme UI** â€“ Modern, user-friendly design.
- **Responsive Layout** â€“ Works on all devices.

## ğŸ“‚ Project Structure
```
ğŸ“¦ ai-web-scraper
â”œâ”€â”€ backend
â”‚   â”œâ”€â”€ main.py  # FastAPI server
â”‚   â”œâ”€â”€ utils.py  # Helper functions
â”‚   â”œâ”€â”€ requirements.txt  # Dependencies
â”‚
â”œâ”€â”€ frontend
â”‚   â”œâ”€â”€ index.html  # UI layout
â”‚   â”œâ”€â”€ styles.css  # Styling (dark mode)
â”‚   â”œâ”€â”€ script.js  # Client-side logic
â”‚
â”œâ”€â”€ README.md  # Project documentation
â”œâ”€â”€ .env  # API keys and secrets
```

## ğŸš€ Installation & Setup

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/your-username/ai-web-scraper.git
cd ai-web-scraper
```

### 2ï¸âƒ£ Backend Setup
#### **Install Dependencies**
```bash
cd backend
pip install -r requirements.txt
```
#### **Set Up Environment Variables**
Create a `.env` file in the `backend` folder and add:
```
GROQ_API_KEY=your_groq_api_key_here
```
#### **Run FastAPI Server**
```bash
uvicorn main:app --reload
```

### 3ï¸âƒ£ Frontend Setup
Simply open `frontend/index.html` in a browser or serve it with a local server:
```bash
cd frontend
python -m http.server 8000
```
Then, visit `http://localhost:8000`.

## ğŸ¯ Usage Guide
### **Processing a Web Page**
1. Enter a **valid URL** in the input box.
2. Click **"Process"** to extract text from the webpage.
3. The system will store and index the content for queries.

### **Querying AI**
1. Type your **question** in the query box.
2. Click **"Get Answer"** to retrieve an AI-generated response.
3. View results in the response section.

## ğŸ› ï¸ API Endpoints
### **1ï¸âƒ£ Process URL**
```http
POST /process_url
```
#### Request Body:
```json
{
  "url": "https://example.com"
}
```
#### Response:
```json
{
  "message": "URL processed successfully"
}
```

### **2ï¸âƒ£ Query System**
```http
POST /query
```
#### Request Body:
```json
{
  "prompt": "What is this article about?"
}
```
#### Response:
```json
{
  "answer": "The article discusses AI-powered web scraping...",
  "response_time": 0.82
}
```

## ğŸ“Œ Future Improvements
- ğŸ“Œ **Support for Multiple Pages** â€“ Allow users to process multiple URLs.
- ğŸ“Œ **Authentication** â€“ Secure API with user authentication.
- ğŸ“Œ **Django Migration** â€“ Move backend to Django for better scalability.

